{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1SNAaHWGftL6O2xHiFx_1l-Hxf20wVGLr","timestamp":1669522115231}],"authorship_tag":"ABX9TyPjCvyZZgZs6a0ARakF0Jfp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install comet_ml -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KK6sIUdKVtr","executionInfo":{"status":"ok","timestamp":1669522221498,"user_tz":300,"elapsed":27651,"user":{"displayName":"An Ni Wu","userId":"11154020135232560694"}},"outputId":"08352143-b35c-4332-9a90-1c54e8b2588e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 441 kB 7.6 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 1.2 MB/s \n","\u001b[K     |████████████████████████████████| 130 kB 13.4 MB/s \n","\u001b[K     |████████████████████████████████| 168 kB 44.2 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 686 kB/s \n","\u001b[K     |████████████████████████████████| 498 kB 30.3 MB/s \n","\u001b[K     |████████████████████████████████| 140 kB 39.1 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 22.4 MB/s \n","\u001b[K     |████████████████████████████████| 168 kB 29.9 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 31.7 MB/s \n","\u001b[K     |████████████████████████████████| 166 kB 22.0 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 14.3 MB/s \n","\u001b[K     |████████████████████████████████| 162 kB 51.2 MB/s \n","\u001b[K     |████████████████████████████████| 158 kB 41.2 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 9.9 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 17.0 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 24.8 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 45.1 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 14.9 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 14.9 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 18.9 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 48.4 MB/s \n","\u001b[?25h  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nbuhXwuKbSw","executionInfo":{"status":"ok","timestamp":1669522245704,"user_tz":300,"elapsed":24222,"user":{"displayName":"An Ni Wu","userId":"11154020135232560694"}},"outputId":"c568e557-2dcb-4447-c11c-a86589dee333"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","RANDOM_SEED = 42\n","\n","# Load data\n","data_df = pd.read_csv('/content/drive/MyDrive/MMD6020_Final_Project/data/processed/chbmit_preprocessed_data.csv') \n","\n","# Separate X and y\n","y = data_df['Outcome']\n","X = data_df.drop(['Outcome'], axis=1)\n","\n","# Split into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, stratify=y, random_state=RANDOM_SEED)"],"metadata":{"id":"PuQyr3wvKn4v","executionInfo":{"status":"ok","timestamp":1669522275573,"user_tz":300,"elapsed":29889,"user":{"displayName":"An Ni Wu","userId":"11154020135232560694"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","from getpass import getpass\n","\n","API_KEY = getpass('Enter Comet. ml API key:')\n","\n","os.environ['COMET_API_KEY'] = API_KEY"],"metadata":{"id":"yFTjrLnhQDzm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669522280556,"user_tz":300,"elapsed":5007,"user":{"displayName":"An Ni Wu","userId":"11154020135232560694"}},"outputId":"a09db82c-197e-41bb-ff4a-bef4b72f0712"},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Comet. ml API key:··········\n"]}]},{"cell_type":"markdown","source":["# AdaBoost"],"metadata":{"id":"oGOU92iYMmTK"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":522},"id":"C2JNg51rJJyj","outputId":"4987ba61-f5fb-4ffc-9e82-c7164bc2eebf","executionInfo":{"status":"error","timestamp":1669522982580,"user_tz":300,"elapsed":690848,"user":{"displayName":"An Ni Wu","userId":"11154020135232560694"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["COMET WARNING: Passing Experiment through Optimizer constructor is deprecated; pass them to Optimizer.get_experiments or Optimizer.next\n","COMET INFO: COMET_OPTIMIZER_ID=4540cbed00334cc893959e28a070cc73\n","COMET INFO: Using optimizer config: {'algorithm': 'bayes', 'configSpaceSize': 'infinite', 'endTime': None, 'id': '4540cbed00334cc893959e28a070cc73', 'lastUpdateTime': None, 'maxCombo': 0, 'name': 'Bayes Optimization', 'parameters': {'learning_rate': {'type': 'discrete', 'values': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}, 'n_estimators': {'max': 100, 'min': 20, 'scalingType': 'uniform', 'scaling_type': 'uniform', 'type': 'integer'}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 1000, 'seed': 42}, 'startTime': 12946187475, 'state': {'mode': None, 'seed': None, 'sequence': [], 'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0, 'sequence_retry_count': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 3, 'version': '2.0.1'}\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n","COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n","COMET INFO: Experiment is live on comet.com https://www.comet.com/mmd6020-projet-pratique/adaboost-pca/8af0b66ee4734e62b7bdbc70a638e2d2\n","\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-685565424f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mHyperParametersTuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adaboost-pca\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-685565424f39>\u001b[0m in \u001b[0;36mHyperParametersTuning\u001b[0;34m(project_name, X_train, y_train)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mrun_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-685565424f39>\u001b[0m in \u001b[0;36mrun_search\u001b[0;34m(experiment, model, X, y, cv)\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0;34m\"f1_macro\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0;34m\"roc_auc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m       ], return_train_score=True)\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 289\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 289\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             )\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \"\"\"\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SAMME.R\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         )\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import numpy as np\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n","from sklearn.decomposition import PCA\n","\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.feature_selection import SelectKBest, chi2\n","\n","from imblearn.pipeline import Pipeline\n","\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import StratifiedKFold\n","\n","from comet_ml import Experiment\n","from comet_ml import Optimizer\n","from comet_ml import API\n","\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","\n","def run_search(experiment, model, X, y, cv):\n","  # fit the model on the whole dataset\n","  results = cross_validate(\n","      model, X, y, cv=cv, \n","      scoring=[\n","          \"accuracy\",\n","          \"precision_macro\", \n","          \"recall_macro\", \n","          \"f1_macro\", \n","          \"roc_auc\",\n","      ], return_train_score=True)\n","\n","  for k in results.keys():\n","    scores = results[k]\n","    for idx, score in enumerate(scores):\n","      experiment.log_metrics({f\"cv_{k}\": score}, step=idx)\n","\n","    experiment.log_metrics({f\"cv_mean_{k}\": np.mean(scores)})\n","    experiment.log_metrics({f\"cv_std_{k}\": np.std(scores)})\n","\n","    experiment.log_parameter(\"random_state\", RANDOM_SEED)\n","    \n","def HyperParametersTuning(project_name, X_train, y_train):\n","\n","    # setting the spec for bayes algorithm\n","    spec = {\n","        \"objective\": \"minimize\",\n","        \"metric\": \"loss\",\n","        \"seed\": RANDOM_SEED\n","    }\n","\n","    # setting the parameters we are tuning\n","    model_params = {\n","        \"n_estimators\": {\n","            \"type\": \"integer\",\n","            \"scaling_type\": \"uniform\",\n","            \"min\": 20,\n","            \"max\": 100\n","        },\n","        \"learning_rate\": {\n","            \"type\": \"discrete\",\n","            \"values\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n","        },\n","    }\n","\n","\n","    # defining the configuration dictionary\n","    config_dict = {\n","        \"algorithm\": \"bayes\",\n","        \"spec\": spec, \n","        \"parameters\": model_params,\n","        \"name\": \"Bayes Optimization\", \n","        \"trials\": 3\n","    }\n","\n","    cv = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True) # use 5-fold stratified cv\n","\n","    # initializing the comet ml optimizer\n","    opt = Optimizer(\n","        api_key=os.environ.get('COMET_API_KEY'), # create an env var called 'COMET_API_KEY' containing the API key\n","        config=config_dict,\n","        project_name=project_name, # change name to model-selector\n","        workspace=\"mmd6020-projet-pratique\")\n","\n","   \n","    for experiment in opt.get_experiments():\n","\n","        n_estimators   = experiment.get_parameter(\"n_estimators\")\n","        learning_rate  = experiment.get_parameter(\"learning_rate\")\n","\n","        selector = PCA(n_components=12) # change selector for feature selection\n","\n","        clf_adaboost = AdaBoostClassifier(\n","            n_estimators=n_estimators,\n","            learning_rate=learning_rate,\n","            random_state=RANDOM_SEED)\n","\n","        # Pipeline\n","        steps = [('selector', selector), (\"clf_adaboost\", clf_adaboost)]\n","        pipeline = Pipeline(steps=steps)\n","\n","        run_search(experiment, pipeline, X_train, y_train, cv)\n","\n","        pipeline.fit(X_train, y_train)\n","        \n","        experiment.log_parameter(\"random_state\", RANDOM_SEED)\n","        experiment.end()\n","  \n","HyperParametersTuning(\"adaboost-pca\", X_train, y_train) "]},{"cell_type":"markdown","source":["# Logistic Regression"],"metadata":{"id":"4u9f2dPpPCs1"}},{"cell_type":"code","source":["import numpy as np\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n","from sklearn.decomposition import PCA\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_selection import SelectKBest, chi2\n","\n","from imblearn.pipeline import Pipeline\n","\n","from sklearn.model_selection import cross_validate\n","from sklearn.model_selection import StratifiedKFold\n","\n","from comet_ml import Experiment\n","from comet_ml import Optimizer\n","from comet_ml import API\n","\n","RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","\n","def run_search(experiment, model, X, y, cv):\n","  # fit the model on the whole dataset\n","  results = cross_validate(\n","      model, X, y, cv=cv, \n","      scoring=[\n","          \"accuracy\",\n","          \"precision_macro\", \n","          \"recall_macro\", \n","          \"f1_macro\", \n","          \"roc_auc\",\n","      ], return_train_score=True)\n","\n","  for k in results.keys():\n","    scores = results[k]\n","    for idx, score in enumerate(scores):\n","      experiment.log_metrics({f\"cv_{k}\": score}, step=idx)\n","\n","    experiment.log_metrics({f\"cv_mean_{k}\": np.mean(scores)})\n","    experiment.log_metrics({f\"cv_std_{k}\": np.std(scores)})\n","\n","    experiment.log_parameter(\"random_state\", RANDOM_SEED)\n","    \n","def HyperParametersTuning(project_name, X_train, y_train):\n","\n","    # setting the spec for bayes algorithm\n","    spec = {\n","        \"objective\": \"minimize\",\n","        \"metric\": \"loss\",\n","        \"seed\": RANDOM_SEED\n","    }\n","\n","    model_params = {\n","        \"penalty\": {\n","            \"type\": \"categorical\",\n","            \"values\": [\"l1\", \"l2\", \"elasticnet\", \"none\"]\n","        },\n","        # \"solver\": {\n","        #     \"type\": \"categorical\",\n","        #     \"values\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n","        # },\n","        \"C\": {\n","            \"type\": \"discrete\",\n","            \"values\": [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50, 100]\n","        },\n","        \"max_iter\": {\n","            \"type\": \"integer\",\n","            \"scaling_type\": \"uniform\",\n","            \"min\": 100,\n","            \"max\": 1000\n","        },\n","        \"l1_ratio\": {\n","            \"type\": \"discrete\",\n","            \"values\": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n","        },\n","    }\n","\n","\n","    # defining the configuration dictionary\n","    config_dict = {\n","        \"algorithm\": \"bayes\",\n","        \"spec\": spec, \n","        \"parameters\": model_params,\n","        \"name\": \"Bayes Optimization\", \n","        \"trials\": 3\n","    }\n","\n","    cv = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True) # use 5-fold stratified cv\n","\n","    # initializing the comet ml optimizer\n","    opt = Optimizer(\n","        api_key=os.environ.get('COMET_API_KEY'), # create an env var called 'COMET_API_KEY' containing the API key\n","        config=config_dict,\n","        project_name=project_name, # change name to model-selector\n","        workspace=\"mmd6020-projet-pratique\")\n","\n","   \n","    for experiment in opt.get_experiments():\n","\n","        penalty      = experiment.get_parameter(\"penalty\")\n","        # solver       = experiment.get_parameter(\"solver\")\n","        C            = experiment.get_parameter(\"C\")\n","        max_iter     = experiment.get_parameter(\"max_iter\")\n","        l1_ratio     = experiment.get_parameter(\"l1_ratio\")\n","        \n","        selector = PCA(n_components=12) # change selector for feature selection\n","\n","        clf_logreg = LogisticRegression(\n","            penalty=penalty,\n","            solver='saga',\n","            C=C,\n","            max_iter=max_iter,\n","            class_weight=None,\n","            l1_ratio=l1_ratio,\n","            random_state=RANDOM_SEED)\n","\n","\n","        # Pipeline\n","        steps = [('selector', selector), (\"logreg\", clf_logreg)]\n","        pipeline = Pipeline(steps=steps)\n","\n","        run_search(experiment, pipeline, X_train, y_train, cv)\n","\n","        pipeline.fit(X_train, y_train)\n","        \n","        experiment.log_parameter(\"random_state\", RANDOM_SEED)\n","        experiment.end()\n","  \n","HyperParametersTuning(\"logreg-pca\", X_train, y_train) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yv7h2gulUMnm","outputId":"fddf9ba8-ec6a-47ba-e4a7-220fa95f3cca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["COMET WARNING: Passing Experiment through Optimizer constructor is deprecated; pass them to Optimizer.get_experiments or Optimizer.next\n","COMET INFO: COMET_OPTIMIZER_ID=d79e0b9cf9aa44a9ac4b9d448e30db4a\n","COMET INFO: Using optimizer config: {'algorithm': 'bayes', 'configSpaceSize': 'infinite', 'endTime': None, 'id': 'd79e0b9cf9aa44a9ac4b9d448e30db4a', 'lastUpdateTime': None, 'maxCombo': 0, 'name': 'Bayes Optimization', 'parameters': {'C': {'type': 'discrete', 'values': [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50, 100]}, 'l1_ratio': {'type': 'discrete', 'values': [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]}, 'max_iter': {'max': 1000, 'min': 100, 'scalingType': 'uniform', 'scaling_type': 'uniform', 'type': 'integer'}, 'penalty': {'type': 'categorical', 'values': ['l1', 'l2', 'elasticnet', 'none']}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 1000, 'seed': 42}, 'startTime': 58266295606, 'state': {'mode': None, 'seed': None, 'sequence': [], 'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0, 'sequence_retry_count': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 3, 'version': '2.0.1'}\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n","COMET INFO: ---------------------------\n","COMET INFO: Comet.ml Experiment Summary\n","COMET INFO: ---------------------------\n","COMET INFO:   Data:\n","COMET INFO:     display_summary_level : 1\n","COMET INFO:     url                   : https://www.comet.com/mmd6020-projet-pratique/logreg-pca/efd459e533f14a36ad773fe983148186\n","COMET INFO:   Others:\n","COMET INFO:     notebook_url           : https://colab.research.google.com/notebook#fileId=1NwQq2mck4boXjEnTjJHY0wN_UrmH4d2l\n","COMET INFO:     optimizer_count        : 1\n","COMET INFO:     optimizer_id           : b13d49d844ce447db117baf5ea0dac2c\n","COMET INFO:     optimizer_metric       : loss\n","COMET INFO:     optimizer_metric_value : 1\n","COMET INFO:     optimizer_name         : Bayes Optimization\n","COMET INFO:     optimizer_objective    : minimum\n","COMET INFO:     optimizer_parameters   : {\"C\": 0.05, \"l1_ratio\": 0.6, \"max_iter\": 710, \"penalty\": \"elasticnet\"}\n","COMET INFO:     optimizer_pid          : c04b1720622d6fb8510cabc3aaf37e41f2dae681\n","COMET INFO:     optimizer_process      : 74\n","COMET INFO:     optimizer_trial        : 1\n","COMET INFO:     optimizer_version      : 2.0.1\n","COMET INFO:   Parameters:\n","COMET INFO:     C        : 0.05\n","COMET INFO:     l1_ratio : 0.6\n","COMET INFO:     max_iter : 710\n","COMET INFO:     penalty  : elasticnet\n","COMET INFO:   Uploads:\n","COMET INFO:     environment details : 1\n","COMET INFO:     filename            : 1\n","COMET INFO:     installed packages  : 1\n","COMET INFO:     notebook            : 2\n","COMET INFO:     os packages         : 1\n","COMET INFO:     source_code         : 1\n","COMET INFO: ---------------------------\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n","COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n","COMET INFO: Experiment is live on comet.com https://www.comet.com/mmd6020-projet-pratique/logreg-pca/c837864a0b6447d392a307844fa7fe4e\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n","COMET INFO: ---------------------------\n","COMET INFO: Comet.ml Experiment Summary\n","COMET INFO: ---------------------------\n","COMET INFO:   Data:\n","COMET INFO:     display_summary_level : 1\n","COMET INFO:     url                   : https://www.comet.com/mmd6020-projet-pratique/logreg-pca/c837864a0b6447d392a307844fa7fe4e\n","COMET INFO:   Metrics [count] (min, max):\n","COMET INFO:     cv_fit_time [5]               : (28.68104863166809, 213.21485209465027)\n","COMET INFO:     cv_mean_fit_time              : 93.67349553108215\n","COMET INFO:     cv_mean_score_time            : 1.187117862701416\n","COMET INFO:     cv_mean_test_accuracy         : 0.4999425145766609\n","COMET INFO:     cv_mean_test_f1_macro         : 0.399942325661269\n","COMET INFO:     cv_mean_test_precision_macro  : 0.34994263997913894\n","COMET INFO:     cv_mean_test_recall_macro     : 0.49994277829343314\n","COMET INFO:     cv_mean_test_roc_auc          : 0.4999051853193094\n","COMET INFO:     cv_mean_train_accuracy        : 0.5004720692368214\n","COMET INFO:     cv_mean_train_f1_macro        : 0.40047121216667636\n","COMET INFO:     cv_mean_train_precision_macro : 0.3504720013342994\n","COMET INFO:     cv_mean_train_recall_macro    : 0.5004719370289538\n","COMET INFO:     cv_mean_train_roc_auc         : 0.5006886566469804\n","COMET INFO:     cv_score_time [5]             : (1.1011228561401367, 1.2715539932250977)\n","COMET INFO:     cv_std_fit_time               : 78.27782468970767\n","COMET INFO:     cv_std_score_time             : 0.06985856072174715\n","COMET INFO:     cv_std_test_accuracy          : 0.0015170422843074848\n","COMET INFO:     cv_std_test_f1_macro          : 0.0815933447973013\n","COMET INFO:     cv_std_test_precision_macro   : 0.12241388683308652\n","COMET INFO:     cv_std_test_recall_macro      : 0.0015170530818234933\n","COMET INFO:     cv_std_test_roc_auc           : 0.001051693875501788\n","COMET INFO:     cv_std_train_accuracy         : 0.0013396712947731197\n","COMET INFO:     cv_std_train_f1_macro         : 0.08223554884095\n","COMET INFO:     cv_std_train_precision_macro  : 0.12305836863276091\n","COMET INFO:     cv_std_train_recall_macro     : 0.0013397184116071285\n","COMET INFO:     cv_std_train_roc_auc          : 0.0010624263728810516\n","COMET INFO:     cv_test_accuracy [5]          : (0.4974608397110364, 0.502253057721193)\n","COMET INFO:     cv_test_f1_macro [5]          : (0.33333274464440243, 0.5022529368756619)\n","COMET INFO:     cv_test_precision_macro [5]   : (0.24999933772553756, 0.5022530585956937)\n","COMET INFO:     cv_test_recall_macro [5]      : (0.4974608350477441, 0.5022530564194219)\n","COMET INFO:     cv_test_roc_auc [5]           : (0.4981102569781305, 0.5014156696184165)\n","COMET INFO:     cv_train_accuracy [5]         : (0.4992688489934753, 0.5030908349161693)\n","COMET INFO:     cv_train_f1_macro [5]         : (0.3333333333333333, 0.5030904034551187)\n","COMET INFO:     cv_train_precision_macro [5]  : (0.25, 0.503090846901544)\n","COMET INFO:     cv_train_recall_macro [5]     : (0.4992688489934753, 0.5030908361512934)\n","COMET INFO:     cv_train_roc_auc [5]          : (0.5, 0.5027431329679378)\n","COMET INFO:   Others:\n","COMET INFO:     notebook_url           : https://colab.research.google.com/notebook#fileId=1NwQq2mck4boXjEnTjJHY0wN_UrmH4d2l\n","COMET INFO:     optimizer_count        : 1\n","COMET INFO:     optimizer_id           : d79e0b9cf9aa44a9ac4b9d448e30db4a\n","COMET INFO:     optimizer_metric       : loss\n","COMET INFO:     optimizer_metric_value : 1\n","COMET INFO:     optimizer_name         : Bayes Optimization\n","COMET INFO:     optimizer_objective    : minimum\n","COMET INFO:     optimizer_parameters   : {\"C\": 5, \"l1_ratio\": 0, \"max_iter\": 211, \"penalty\": \"l1\"}\n","COMET INFO:     optimizer_pid          : d7c830c229f58ce64cdc5814f47c775f9640ea6b\n","COMET INFO:     optimizer_process      : 74\n","COMET INFO:     optimizer_trial        : 1\n","COMET INFO:     optimizer_version      : 2.0.1\n","COMET INFO:   Parameters:\n","COMET INFO:     C            : 5\n","COMET INFO:     l1_ratio     : 0\n","COMET INFO:     max_iter     : 211\n","COMET INFO:     penalty      : l1\n","COMET INFO:     random_state : 42\n","COMET INFO:   Uploads:\n","COMET INFO:     environment details : 1\n","COMET INFO:     filename            : 1\n","COMET INFO:     installed packages  : 1\n","COMET INFO:     notebook            : 2\n","COMET INFO:     os packages         : 1\n","COMET INFO:     source_code         : 1\n","COMET INFO: ---------------------------\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n","COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n","COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n","COMET INFO: Experiment is live on comet.com https://www.comet.com/mmd6020-projet-pratique/logreg-pca/a4357d00c119484aa51ef565412f7d0e\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n","COMET INFO: ---------------------------\n","COMET INFO: Comet.ml Experiment Summary\n","COMET INFO: ---------------------------\n","COMET INFO:   Data:\n","COMET INFO:     display_summary_level : 1\n","COMET INFO:     url                   : https://www.comet.com/mmd6020-projet-pratique/logreg-pca/a4357d00c119484aa51ef565412f7d0e\n","COMET INFO:   Metrics [count] (min, max):\n","COMET INFO:     cv_fit_time [5]               : (31.71669626235962, 242.96701383590698)\n","COMET INFO:     cv_mean_fit_time              : 106.19305601119996\n","COMET INFO:     cv_mean_score_time            : 1.2213937759399414\n","COMET INFO:     cv_mean_test_accuracy         : 0.4999425145766609\n","COMET INFO:     cv_mean_test_f1_macro         : 0.399942325661269\n","COMET INFO:     cv_mean_test_precision_macro  : 0.34994263997913894\n","COMET INFO:     cv_mean_test_recall_macro     : 0.49994277829343314\n","COMET INFO:     cv_mean_test_roc_auc          : 0.4999051853193094\n","COMET INFO:     cv_mean_train_accuracy        : 0.5004720692368214\n","COMET INFO:     cv_mean_train_f1_macro        : 0.40047121216667636\n","COMET INFO:     cv_mean_train_precision_macro : 0.3504720013342994\n","COMET INFO:     cv_mean_train_recall_macro    : 0.5004719370289538\n","COMET INFO:     cv_mean_train_roc_auc         : 0.5006886566469804\n","COMET INFO:     cv_score_time [5]             : (1.1473748683929443, 1.3319053649902344)\n","COMET INFO:     cv_std_fit_time               : 89.23449951813966\n","COMET INFO:     cv_std_score_time             : 0.07785620945404363\n","COMET INFO:     cv_std_test_accuracy          : 0.0015170422843074848\n","COMET INFO:     cv_std_test_f1_macro          : 0.0815933447973013\n","COMET INFO:     cv_std_test_precision_macro   : 0.12241388683308652\n","COMET INFO:     cv_std_test_recall_macro      : 0.0015170530818234933\n","COMET INFO:     cv_std_test_roc_auc           : 0.001051693875501788\n","COMET INFO:     cv_std_train_accuracy         : 0.0013396712947731197\n","COMET INFO:     cv_std_train_f1_macro         : 0.08223554884095\n","COMET INFO:     cv_std_train_precision_macro  : 0.12305836863276091\n","COMET INFO:     cv_std_train_recall_macro     : 0.0013397184116071285\n","COMET INFO:     cv_std_train_roc_auc          : 0.0010624263728810516\n","COMET INFO:     cv_test_accuracy [5]          : (0.4974608397110364, 0.502253057721193)\n","COMET INFO:     cv_test_f1_macro [5]          : (0.33333274464440243, 0.5022529368756619)\n","COMET INFO:     cv_test_precision_macro [5]   : (0.24999933772553756, 0.5022530585956937)\n","COMET INFO:     cv_test_recall_macro [5]      : (0.4974608350477441, 0.5022530564194219)\n","COMET INFO:     cv_test_roc_auc [5]           : (0.4981102569781305, 0.5014156696184165)\n","COMET INFO:     cv_train_accuracy [5]         : (0.4992688489934753, 0.5030908349161693)\n","COMET INFO:     cv_train_f1_macro [5]         : (0.3333333333333333, 0.5030904034551187)\n","COMET INFO:     cv_train_precision_macro [5]  : (0.25, 0.503090846901544)\n","COMET INFO:     cv_train_recall_macro [5]     : (0.4992688489934753, 0.5030908361512934)\n","COMET INFO:     cv_train_roc_auc [5]          : (0.5, 0.5027431329679378)\n","COMET INFO:   Others:\n","COMET INFO:     notebook_url           : https://colab.research.google.com/notebook#fileId=1NwQq2mck4boXjEnTjJHY0wN_UrmH4d2l\n","COMET INFO:     optimizer_count        : 2\n","COMET INFO:     optimizer_id           : d79e0b9cf9aa44a9ac4b9d448e30db4a\n","COMET INFO:     optimizer_metric       : loss\n","COMET INFO:     optimizer_metric_value : 1\n","COMET INFO:     optimizer_name         : Bayes Optimization\n","COMET INFO:     optimizer_objective    : minimum\n","COMET INFO:     optimizer_parameters   : {\"C\": 5, \"l1_ratio\": 0, \"max_iter\": 211, \"penalty\": \"l1\"}\n","COMET INFO:     optimizer_pid          : d7c830c229f58ce64cdc5814f47c775f9640ea6b\n","COMET INFO:     optimizer_process      : 74\n","COMET INFO:     optimizer_trial        : 2\n","COMET INFO:     optimizer_version      : 2.0.1\n","COMET INFO:   Parameters:\n","COMET INFO:     C            : 5\n","COMET INFO:     l1_ratio     : 0\n","COMET INFO:     max_iter     : 211\n","COMET INFO:     penalty      : l1\n","COMET INFO:     random_state : 42\n","COMET INFO:   Uploads:\n","COMET INFO:     environment details : 1\n","COMET INFO:     filename            : 1\n","COMET INFO:     installed packages  : 1\n","COMET INFO:     notebook            : 2\n","COMET INFO:     os packages         : 1\n","COMET INFO:     source_code         : 1\n","COMET INFO: ---------------------------\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET INFO: Uploading 1 metrics, params and output messages\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n","COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n","COMET INFO: Experiment is live on comet.com https://www.comet.com/mmd6020-projet-pratique/logreg-pca/711f5236a8fa43c29b119142ba62614a\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n","  \"(penalty={})\".format(self.penalty)\n","COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n","COMET INFO: ---------------------------\n","COMET INFO: Comet.ml Experiment Summary\n","COMET INFO: ---------------------------\n","COMET INFO:   Data:\n","COMET INFO:     display_summary_level : 1\n","COMET INFO:     url                   : https://www.comet.com/mmd6020-projet-pratique/logreg-pca/711f5236a8fa43c29b119142ba62614a\n","COMET INFO:   Metrics [count] (min, max):\n","COMET INFO:     cv_fit_time [5]               : (30.493127822875977, 234.50375938415527)\n","COMET INFO:     cv_mean_fit_time              : 104.2515606880188\n","COMET INFO:     cv_mean_score_time            : 1.2228349685668944\n","COMET INFO:     cv_mean_test_accuracy         : 0.4999425145766609\n","COMET INFO:     cv_mean_test_f1_macro         : 0.399942325661269\n","COMET INFO:     cv_mean_test_precision_macro  : 0.34994263997913894\n","COMET INFO:     cv_mean_test_recall_macro     : 0.49994277829343314\n","COMET INFO:     cv_mean_test_roc_auc          : 0.4999051853193094\n","COMET INFO:     cv_mean_train_accuracy        : 0.5004720692368214\n","COMET INFO:     cv_mean_train_f1_macro        : 0.40047121216667636\n","COMET INFO:     cv_mean_train_precision_macro : 0.3504720013342994\n","COMET INFO:     cv_mean_train_recall_macro    : 0.5004719370289538\n","COMET INFO:     cv_mean_train_roc_auc         : 0.5006886566469804\n","COMET INFO:     cv_score_time [5]             : (1.1459496021270752, 1.3308866024017334)\n","COMET INFO:     cv_std_fit_time               : 87.03133008852738\n","COMET INFO:     cv_std_score_time             : 0.07758563768210142\n","COMET INFO:     cv_std_test_accuracy          : 0.0015170422843074848\n","COMET INFO:     cv_std_test_f1_macro          : 0.0815933447973013\n","COMET INFO:     cv_std_test_precision_macro   : 0.12241388683308652\n","COMET INFO:     cv_std_test_recall_macro      : 0.0015170530818234933\n","COMET INFO:     cv_std_test_roc_auc           : 0.001051693875501788\n","COMET INFO:     cv_std_train_accuracy         : 0.0013396712947731197\n","COMET INFO:     cv_std_train_f1_macro         : 0.08223554884095\n","COMET INFO:     cv_std_train_precision_macro  : 0.12305836863276091\n","COMET INFO:     cv_std_train_recall_macro     : 0.0013397184116071285\n","COMET INFO:     cv_std_train_roc_auc          : 0.0010624263728810516\n","COMET INFO:     cv_test_accuracy [5]          : (0.4974608397110364, 0.502253057721193)\n","COMET INFO:     cv_test_f1_macro [5]          : (0.33333274464440243, 0.5022529368756619)\n","COMET INFO:     cv_test_precision_macro [5]   : (0.24999933772553756, 0.5022530585956937)\n","COMET INFO:     cv_test_recall_macro [5]      : (0.4974608350477441, 0.5022530564194219)\n","COMET INFO:     cv_test_roc_auc [5]           : (0.4981102569781305, 0.5014156696184165)\n","COMET INFO:     cv_train_accuracy [5]         : (0.4992688489934753, 0.5030908349161693)\n","COMET INFO:     cv_train_f1_macro [5]         : (0.3333333333333333, 0.5030904034551187)\n","COMET INFO:     cv_train_precision_macro [5]  : (0.25, 0.503090846901544)\n","COMET INFO:     cv_train_recall_macro [5]     : (0.4992688489934753, 0.5030908361512934)\n","COMET INFO:     cv_train_roc_auc [5]          : (0.5, 0.5027431329679378)\n","COMET INFO:   Others:\n","COMET INFO:     notebook_url           : https://colab.research.google.com/notebook#fileId=1NwQq2mck4boXjEnTjJHY0wN_UrmH4d2l\n","COMET INFO:     optimizer_count        : 3\n","COMET INFO:     optimizer_id           : d79e0b9cf9aa44a9ac4b9d448e30db4a\n","COMET INFO:     optimizer_metric       : loss\n","COMET INFO:     optimizer_metric_value : 1\n","COMET INFO:     optimizer_name         : Bayes Optimization\n","COMET INFO:     optimizer_objective    : minimum\n","COMET INFO:     optimizer_parameters   : {\"C\": 5, \"l1_ratio\": 0, \"max_iter\": 211, \"penalty\": \"l1\"}\n","COMET INFO:     optimizer_pid          : d7c830c229f58ce64cdc5814f47c775f9640ea6b\n","COMET INFO:     optimizer_process      : 74\n","COMET INFO:     optimizer_trial        : 3\n","COMET INFO:     optimizer_version      : 2.0.1\n","COMET INFO:   Parameters:\n","COMET INFO:     C            : 5\n","COMET INFO:     l1_ratio     : 0\n","COMET INFO:     max_iter     : 211\n","COMET INFO:     penalty      : l1\n","COMET INFO:     random_state : 42\n","COMET INFO:   Uploads:\n","COMET INFO:     environment details : 1\n","COMET INFO:     filename            : 1\n","COMET INFO:     installed packages  : 1\n","COMET INFO:     notebook            : 2\n","COMET INFO:     os packages         : 1\n","COMET INFO:     source_code         : 1\n","COMET INFO: ---------------------------\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET INFO: Uploading 1 metrics, params and output messages\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n","COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n","COMET INFO: Experiment is live on comet.com https://www.comet.com/mmd6020-projet-pratique/logreg-pca/a907c34e73bc45558f9bd7e5ad7cedd3\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","COMET INFO: Optimizer metrics is 'loss' but no logged values found. Experiment ignored in sweep.\n","COMET WARNING: Couldn't retrieve Google Colab notebook content\n","COMET INFO: ---------------------------\n","COMET INFO: Comet.ml Experiment Summary\n","COMET INFO: ---------------------------\n","COMET INFO:   Data:\n","COMET INFO:     display_summary_level : 1\n","COMET INFO:     url                   : https://www.comet.com/mmd6020-projet-pratique/logreg-pca/a907c34e73bc45558f9bd7e5ad7cedd3\n","COMET INFO:   Metrics [count] (min, max):\n","COMET INFO:     cv_fit_time [5]               : (437.6639175415039, 610.1059875488281)\n","COMET INFO:     cv_mean_fit_time              : 521.6499782562256\n","COMET INFO:     cv_mean_score_time            : 1.281561803817749\n","COMET INFO:     cv_mean_test_accuracy         : 0.5021460341680641\n","COMET INFO:     cv_mean_test_f1_macro         : 0.5021449911264864\n","COMET INFO:     cv_mean_test_precision_macro  : 0.5021460463488296\n","COMET INFO:     cv_mean_test_recall_macro     : 0.502146034587037\n","COMET INFO:     cv_mean_test_roc_auc          : 0.5018335541130694\n","COMET INFO:     cv_mean_train_accuracy        : 0.5031420949595614\n","COMET INFO:     cv_mean_train_f1_macro        : 0.5031413130478396\n","COMET INFO:     cv_mean_train_precision_macro : 0.5031421139034349\n","COMET INFO:     cv_mean_train_recall_macro    : 0.5031420950090387\n","COMET INFO:     cv_mean_train_roc_auc         : 0.5031207386203935\n","COMET INFO:     cv_score_time [5]             : (1.2534337043762207, 1.3241276741027832)\n","COMET INFO:     cv_std_fit_time               : 67.64736342919802\n","COMET INFO:     cv_std_score_time             : 0.023918547186615003\n","COMET INFO:     cv_std_test_accuracy          : 0.0009295436352341532\n","COMET INFO:     cv_std_test_f1_macro          : 0.0009303764685197068\n","COMET INFO:     cv_std_test_precision_macro   : 0.0009295382125722199\n","COMET INFO:     cv_std_test_recall_macro      : 0.0009295447685382933\n","COMET INFO:     cv_std_test_roc_auc           : 0.0005029280806802241\n","COMET INFO:     cv_std_train_accuracy         : 0.00037176565618333063\n","COMET INFO:     cv_std_train_f1_macro         : 0.0003720618913794322\n","COMET INFO:     cv_std_train_precision_macro  : 0.0003717621244915888\n","COMET INFO:     cv_std_train_recall_macro     : 0.0003717664834074117\n","COMET INFO:     cv_std_train_roc_auc          : 0.000549832362832146\n","COMET INFO:     cv_test_accuracy [5]          : (0.5009205615027802, 0.5037842362783355)\n","COMET INFO:     cv_test_f1_macro [5]          : (0.5009176802407277, 0.5037842080716795)\n","COMET INFO:     cv_test_precision_macro [5]   : (0.5009205763761769, 0.5037842365011531)\n","COMET INFO:     cv_test_recall_macro [5]      : (0.500920555141224, 0.5037842356502761)\n","COMET INFO:     cv_test_roc_auc [5]           : (0.5011266271455193, 0.5026774729736234)\n","COMET INFO:     cv_train_accuracy [5]         : (0.502792811408075, 0.5038074158845205)\n","COMET INFO:     cv_train_f1_macro [5]         : (0.5027902531944786, 0.5038066376508341)\n","COMET INFO:     cv_train_precision_macro [5]  : (0.502792868886877, 0.5038074414559692)\n","COMET INFO:     cv_train_recall_macro [5]     : (0.502792811408075, 0.5038074175442179)\n","COMET INFO:     cv_train_roc_auc [5]          : (0.5022934910546216, 0.5038503143918955)\n","COMET INFO:   Others:\n","COMET INFO:     notebook_url           : https://colab.research.google.com/notebook#fileId=1NwQq2mck4boXjEnTjJHY0wN_UrmH4d2l\n","COMET INFO:     optimizer_count        : 4\n","COMET INFO:     optimizer_id           : d79e0b9cf9aa44a9ac4b9d448e30db4a\n","COMET INFO:     optimizer_metric       : loss\n","COMET INFO:     optimizer_metric_value : 1\n","COMET INFO:     optimizer_name         : Bayes Optimization\n","COMET INFO:     optimizer_objective    : minimum\n","COMET INFO:     optimizer_parameters   : {\"C\": 100, \"l1_ratio\": 0.2, \"max_iter\": 594, \"penalty\": \"none\"}\n","COMET INFO:     optimizer_pid          : 5e5918c9e095acef396b8c8c9dbfd75d1d0d229b\n","COMET INFO:     optimizer_process      : 74\n","COMET INFO:     optimizer_trial        : 1\n","COMET INFO:     optimizer_version      : 2.0.1\n","COMET INFO:   Parameters:\n","COMET INFO:     C            : 100\n","COMET INFO:     l1_ratio     : 0.2\n","COMET INFO:     max_iter     : 594\n","COMET INFO:     penalty      : none\n","COMET INFO:     random_state : 42\n","COMET INFO:   Uploads:\n","COMET INFO:     environment details : 1\n","COMET INFO:     filename            : 1\n","COMET INFO:     installed packages  : 1\n","COMET INFO:     notebook            : 1\n","COMET INFO:     os packages         : 1\n","COMET INFO:     source_code         : 1\n","COMET INFO: ---------------------------\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET INFO: Uploading 1 metrics, params and output messages\n","COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n","COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n","COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n","COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n","COMET INFO: Experiment is live on comet.com https://www.comet.com/mmd6020-projet-pratique/logreg-pca/f11b2850bdd3496391c3b8f40a61ddfb\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  ConvergenceWarning,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1479: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n","  \"(penalty={})\".format(self.penalty)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n","  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n"]}]},{"cell_type":"markdown","source":["# Random Forest"],"metadata":{"id":"cYsVHKs-O9tF"}},{"cell_type":"markdown","source":["# KNN"],"metadata":{"id":"Ee5-FRgPPF2a"}},{"cell_type":"markdown","source":["# XGBoost"],"metadata":{"id":"GAuCi47pPJuY"}},{"cell_type":"code","source":["# Use \"gpu_hist\" for training the model.\n","classi = xgb.XGBClassifier(tree_method=\"gpu_hist\")\n","# Fit the model using predictor X and response y.\n","model_xgb = classi.fit(X_train, y_train)\n","print('Accuracy on training data: ', round(model_xgb.score(X_train,y_train),2))\n","result_xgb = model_xgb.score(X_test,y_test)\n","print('Accuracy on testing data: ', round(result_xgb*100,2))"],"metadata":{"id":"dZBxp45ZtxxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","tuned_parameters = {\n","  'gamma': np.linspace(0, 2, num=50),\n","  'max_depth': np.linspace(0, 12, num=13, dtype=int),\n","    'max_delta_step': np.linspace(0,10, num=11),\n","     'scale_pos_weight': [0.1, 1.0]\n","  }\n","\n","cv = StratifiedKFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n","\n","random_search = RandomizedSearchCV(\n","    xgb.XGBClassifier(tree_method=\"gpu_hist\"), \n","    tuned_parameters, \n","    n_iter=150, \n","    scoring='f1_macro', \n","    cv=cv,\n",")\n","random_search.fit(X_train, y_train);\n","print('Finished!')"],"metadata":{"id":"IbVXESgkt-sP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Meilleur ensemble de paramètres trouvé\")\n","print()\n","print(random_search.best_params_)\n","print(random_search.best_score_)"],"metadata":{"id":"R-DdiqxiuTgk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use \"gpu_hist\" for training the model.\n","classi = xgb.XGBClassifier(tree_method=\"gpu_hist\", scale_pos_weight = random_search.best_params_['scale_pos_weight'], max_depth = random_search.best_params_['max_depth'], max_delta_step = random_search.best_params_['max_delta_step'], gamma = random_search.best_params_['gamma'])\n","# Fit the model using predictor X and response y.\n","model_xgb = classi.fit(X_train, y_train)\n","print('Accuracy on training data: ', round(model_xgb.score(X_train,y_train),2))\n","result_xgb = model_xgb.score(X_test,y_test)\n","print('Accuracy on testing data: ', round(result_xgb*100,2))"],"metadata":{"id":"EH-_KEqZue1m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.calibration import CalibrationDisplay\n","disp_test = CalibrationDisplay.from_estimator(model_xgb,X_test, y_test, n_bins = 105)\n","disp_test.plot(ax=axes[1,1])\n","axes[1,1].set_title(\"Calibration curve\")"],"metadata":{"id":"62suMI7LuvuY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plot ROC"],"metadata":{"id":"HUR05YJiwGeQ"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","\n","colors = ['red', 'blue', 'green', 'yellow', 'orange']\n","\n","def plot_ROC(classifiers_tuple, plot_name, add_random=True):\n","\n","    plt.figure(figsize=(8, 8))\n","\n","    for count, classifier in enumerate(classifiers_tuple):\n","\n","        \n","        clf = classifier[0]\n","        clf_name = classifier[1]\n","        X = classifier[2]\n","        y = classifier[3]\n","\n","        y_pred = clf.predict_proba(X)[:,1]\n","        fpr, tpr, _ = roc_curve(y.ravel(), y_pred.ravel())\n","        roc_auc = auc(fpr, tpr)\n","\n","        plt.plot(fpr, tpr, color=colors[count], label=f\"{clf_name}: AUC = %0.2f\" % roc_auc)\n","        plt.xlabel(\"False Positive Rate\")\n","        plt.ylabel(\"True Positive Rate\")\n","        plt.title(\"ROC Curves\")\n","\n","    if add_random:\n","        plt.plot([0, 1], [0, 1], color=\"black\", label='Random Uniform (AUC = 0.5)', linestyle=\"--\")\n","    \n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.0])\n","    plt.legend(loc=\"lower right\")"],"metadata":{"id":"ZHW4mTSAwP9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifiers = [(clf, 'AdaBoost', X_val, y_val),\n","               (clf3, 'Logistic Regression', X_val, y_val),\n","               (clf4, 'Random Forest', X_val, y_val),\n","               \n","               (model_xgb, 'XGBoost', X_val, y_val)]\n","plot_ROC(classifiers, plot_name='Testing', add_random=True)"],"metadata":{"id":"Hax0EMrSwTEG"},"execution_count":null,"outputs":[]}]}